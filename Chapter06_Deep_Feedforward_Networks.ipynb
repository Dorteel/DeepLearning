{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter06_Deep_Feedforward_Networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMp+T7YMiMxPJynqJ0wwtvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/DeepLearning/blob/master/Chapter06_Deep_Feedforward_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JOmXq0V1HZM",
        "colab_type": "text"
      },
      "source": [
        "Computing Jacobian matrices is not so starightforward in deep learning frameworks since different conventions are used to make things work better.\n",
        "\n",
        "The materials that I have been closely folliwng are [wikipedia pages](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) and two pdfs\n",
        "\n",
        "1. [Computing_Neural_Network_Gradients_Stanford_CS224N](https://github.com/tae898/DeepLearning/raw/master/data/Computing_Neural_Network_Gradients_Stanford_CS224N.pdf)\n",
        "\n",
        "2. [Derivatives_Backpropagation_and_Vectorization_Stanford_CS224N](https://github.com/tae898/DeepLearning/raw/master/data/Derivatives_Backpropagation_and_Vectorization_Stanford_CS224N.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPVeRIzu14FR",
        "colab_type": "text"
      },
      "source": [
        "So this is how I understood.\n",
        "\n",
        "### In the theoretical mathematical point of view:\n",
        "1.   **When it comes to computing gradients of a function** (vector in, scalar out), there is no need to worry about the shapes of the vectors. Most of the time the input vector to the function is a column vector so the the shape of the gradient vector is also a column vector. \n",
        "2.   **When it comes to computing jacobians of a function** (vector in, vector out), the shape is transposed! If the input vector to the function is a column vector and the function outputs a scalar, then the shape of the jacobian is a row vector. Likewise, if the input vector to the function is a row vector and the function outputs a scalar, then the shape of the jacobian should be a column vector. When the input vector to the function has a shape of $\\mathbb{R}^{n \\times 1}$ and the output has a shape of $\\mathbb{R}^{m \\times 1}$, then the shape of the jacobian is $\\mathbb{R}^{m \\times n}$. Likewise, When the input vector to the function has a shape of $\\mathbb{R}^{1 \\times n}$ and the output has a shape of $\\mathbb{R}^{1 \\times m}$, then the shape of the jacobian is $\\mathbb{R}^{m \\times n}$. This is just how the math works. There is no argue with that.\n",
        "\n",
        "\n",
        "### In the deep learning point of view:\n",
        "1. We can do the same for gradients, since there is no shape changes anyways.\n",
        "2. When the input vector to the function has a shape of $\\mathbb{R}^{n \\times 1}$ and the output has a shape of $\\mathbb{R}^{m \\times 1}$, then the shape of the jacobian is $\\mathbb{R}^{m \\times n}$. Likewise, When the input vector to the function has a shape of $\\mathbb{R}^{1 \\times n}$ and the output has a shape of $\\mathbb{R}^{1 \\times m}$, then the shape of the jacobian is $\\mathbb{R}^{m \\times n}$. So far it's the same as doing math on paper with a pen. Now the nasty part starts.\n",
        "    *   When the input vector to the function is a column vector and the function outputs a scalar, then the jacobian should be a column vector! So here jacobian has the same shape as the input vector. The reason why we do this is that when we do gradient descent, we want the following equation to be easy. $\\pmb{b} = \\pmb{b}-\\alpha\\frac{\\partial J}{\\partial \\pmb{b}}$\n",
        "    * When the input vector to the function is a row vector and the function outputs a scalar, the shape of the jacobian is a row vector. The reason is the same as above.\n",
        "    * Now comes the most nasty part. Now we even want to compute $\\frac{\\partial J}{\\partial \\pmb{W}}$, where $\\pmb{W}$ is a $n \\times m$ matrix. Here $J$ is a function whose input is a $n \\times m$ matrix and output is a scalar. Does the jacobian even works in this case? Jacobian is well defined for functions that takes vectors as inputs and outputs. But here input is a matrix. So we are gonna introduce some hacks here, as deep learning always does. Again, to make the equation $\\pmb{W} = \\pmb{W}-\\alpha\\frac{\\partial J}{\\partial \\pmb{W}}$ easy, we will make $\\frac{\\partial J}{\\partial \\pmb{W}}$ have the same shape as $\\pmb{W}$. Read [Computing_Neural_Network_Gradients_Stanford_CS224N](https://github.com/tae898/DeepLearning/raw/master/data/Computing_Neural_Network_Gradients_Stanford_CS224N.pdf) to see how it works.\n",
        "\n",
        "\n",
        "\n",
        "I am super glad that someone has done all those works and put them into tensorflow and pytorch, so that I don't even have to think about them. But since I am person who is always interested in finding out how things work behind the curtain, I'd like to implement a simple example using only numpy."
      ]
    }
  ]
}