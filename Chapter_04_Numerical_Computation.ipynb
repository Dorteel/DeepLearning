{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_04_Numerical_Computation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbCmxYtCE3N+xQpIJIupNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/DeepLearning/blob/master/Chapter_04_Numerical_Computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF2dX8ZHm9w0",
        "colab_type": "text"
      },
      "source": [
        "# 4.1 Overflow and Underflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOnCewbjm8k-",
        "colab_type": "text"
      },
      "source": [
        "As said in the book, most of you deep learning developers / engineers don't have to bother thinking about overflows and underflows. Most of us, including me, use high-level libraries. If you are curious, you can always dig into the low-level libraries where some people have done a lot of work already. Surprisingly you will find some \"hacks\" too, which might not make 100% sense to you, if you are a math-nerd. \n",
        "\n",
        "As of writing this text, 30 July, 2020, most deep learning developers use either tensorflow or pytorch as their main deep learning framework. All of the low-level implementations are done by the contributers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60erpQiW-unw",
        "colab_type": "text"
      },
      "source": [
        "Nonetheless, it' always fun to try out some examples!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yegdZYzB_DKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEJgdT3q_qAm",
        "colab_type": "text"
      },
      "source": [
        "Softmax is a very easy and popular function used throughout deep learning. You can think of it as a genearlized logistic function, which we have learned in [the previous chapter](https://github.com/tae898/DeepLearning/blob/master/Chapter03_Probability_and_Information_Theory.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p88sT_7e-sAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic(scalar):\n",
        "    \"\"\"The logistic function.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    scalar: a float-like\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    logistic: a float-like      \n",
        "    \n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(vec):\n",
        "    \"\"\"Softmax function.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vec: a numpy-array like\n",
        "        a vector-like\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    softmax: a numpy-array like\n",
        "        a vector-like\n",
        "\n",
        "    \"\"\"\n",
        "    return np.exp(vec) / np.exp(vec).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efi1GqGOAcd9",
        "colab_type": "text"
      },
      "source": [
        "The input to the the softmax function should be a vector whose length is more than one.\n",
        "\n",
        "Let's say $x=[-2, 1.5, 0.5]$. When we plug this vector into the softmax function, it returns a probability distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rqV7f05_EgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48dd49ea-9fdf-4056-aaf7-c67a6275fb35"
      },
      "source": [
        "x = [-2, 1.5, 0.5]\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.02159923, 0.71526828, 0.26313249])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BgG7L1GEpAR",
        "colab_type": "text"
      },
      "source": [
        "Each value in the returned vector is a probability and the sum of them should be 1, since it's a probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hME-_pxjFdUH",
        "colab_type": "text"
      },
      "source": [
        "Let's recall the logistic function that we have learned in the previous chapter. It expects a scalar real number as input and outputs a probability, whose value is between 0 and 1. This can be thought of as the softmax function when the input vector has length 2. I will show you below.\n",
        "\n",
        "Let's say \n",
        "\n",
        "$$x = [x_{1}, x_{2}] \\tag{1}$$\n",
        "\n",
        "When we plug this vector into the softmax function, then the output is \n",
        "\n",
        "$[\\frac{e^{x_1}}{e^{x_1} + e^{x_2}}, \\frac{e^{x_2}}{e^{x_1} + e^{x_2}}] \\tag{2}$ This can be re-written as $[\\frac{1}{1 + e^{-(x_1 - x_2)}}, \\frac{1}{1 + e^{x_1 - x_2}} \\tag{3}]$\n",
        "\n",
        "From the softmax function point of view, when the input is $x=[x_1 - x_2, 0]$, what we did with equation (2) and equation (3) are identical. Let's do the math.\n",
        "\n",
        "$[\\frac{e^{x_1-x_2}}{e^{x_1-x_2} + e^{0}}, \\frac{e^{0}}{e^{x_1-x_2} + e^{0}}] = [\\frac{1}{1 + e^{-(x_1 - x_2)}}, \\frac{1}{1 + e^{x_1 - x_2}}] \\tag{4}$\n",
        "\n",
        "This means that when the input to the softmax is a vector of length 2, then we can always make it look like $[t, 0]$, by subtracting the second element. \n",
        "\n",
        "Recall that $logistic(t) = \\frac{1}{1+e^{-t}}$, which is the probability of the first element of equation (3) and (4), when $t=x_1 - x_2$.\n",
        "\n",
        "What this is telling us is that, when the softmax input is a vector of length 2, then we can just simplfy it to the sigmoid function. Having two elements as input to the softmax is redundant. We don't need to calculate the probabilities twice since once we worked out the probability of the first element $p$, the second should be $1-p$ anyways. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DKx78zWQvXc",
        "colab_type": "text"
      },
      "source": [
        "Enough with maths. Let's go back to overflow and underflow.\n",
        "\n",
        "Below cell will throw you an overflow warning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K93oog-tK5w-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "c321cbc2-ba3d-4673-fb3d-b9ce6077c9b7"
      },
      "source": [
        "x = np.array([1e10, 0.1, -123])\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan,  0.,  0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HihDBlSgSajD",
        "colab_type": "text"
      },
      "source": [
        "Obviously calculating $e^{e^{10}}$ results in a very big number.\n",
        "\n",
        "As said in the book, we can subtract the maximum value from every element in the input vector since this doesn't change the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th9JIud1LbT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf10b477-364a-48b5-d349-c25a6ef063d5"
      },
      "source": [
        "x = np.array([1e10, 0.1, -123])\n",
        "x = x - max(x)\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEHXm2dvS8qi",
        "colab_type": "text"
      },
      "source": [
        "Let's try underflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wibJx9WGR6AK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b22932ae-5a85-4824-f0e9-14fb3feb122e"
      },
      "source": [
        "x = np.array([-1e10, -5e10])\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, nan])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7A7D1NmTN-7",
        "colab_type": "text"
      },
      "source": [
        "`invalid value encountered in true_divide` is a warning that numpy throws when it encounters division by 0.\n",
        "\n",
        "This can also be solved by subtracting the maximum value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVY3l5sfSSFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39e85c8b-6b81-40a1-8ffd-f08edd310c9b"
      },
      "source": [
        "x = np.array([-1e10, -5e10])\n",
        "x = x - max(x)\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHV_CBROT-eA",
        "colab_type": "text"
      },
      "source": [
        "logsoftmax mentioned in the book is nothing but the function composition of log and softmax. $log(softmax(\\pmb{x}))$ is what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv9iSfECUq7N",
        "colab_type": "text"
      },
      "source": [
        "Let's say the vector $x=[-1000, 0.1]$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "540NwB-8UZ8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e9183ff-c44a-49ec-e123-255973e52fbf"
      },
      "source": [
        "x = np.array([-1000, 0.1])\n",
        "softmax(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcHCUD6oVQPi",
        "colab_type": "text"
      },
      "source": [
        "As you can see -1000 is already a pretty small number and when this goes to the softmax function, it results in a probability value of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_ruA3yoVF8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0e0f0f59-bdf0-4881-dc28-4408fd1e31fa"
      },
      "source": [
        "np.log(softmax(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-inf,  nan])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3xs-dAPVm4I",
        "colab_type": "text"
      },
      "source": [
        "That's why above error happens!\n",
        "\n",
        "One of the hacks we can do is to add a very small value to the probabilities so that none of them are 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55d46ix8XH0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ead5853-115a-4657-be07-9dcb652a1710"
      },
      "source": [
        "x = np.array([-1000, 0.1])\n",
        "z = softmax(x)\n",
        "z += 1e-100\n",
        "np.log(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-230.2585093,    0.       ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8NhYiw-0jS4",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 Poor Conditioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY-VEhgt0kEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1awFaZMU0k_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import linalg as LA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEnxXAv703ne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99ac8569-7f8e-4b3d-9c5d-604960624720"
      },
      "source": [
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 2., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}