{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter05_Machine_Learning_Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMojqmkB4G0niCpCL7FG2zH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/DeepLearning/blob/master/Chapter05_Machine_Learning_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq-a7I4gkDCs",
        "colab_type": "text"
      },
      "source": [
        "# MLE and MAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4O42H4SSsnH",
        "colab_type": "text"
      },
      "source": [
        "## Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "Consider a set of m examples $\\mathbb{X} = \\{\\pmb{x}^{(1)}, ..., \\pmb{x}^{(m)}\\}$ drawn independently from the true but unknown data generating distribution $p_{data}(\\mathbf{x})$. \n",
        "\n",
        "Let $p_{model}(\\mathbf{x}|\\pmb{\\theta})$ be a parametric family of probability distributions over the same space indexed by $\\pmb{\\theta}$. In other words, $p_{model}(\\mathbf{x}|\\pmb{\\theta})$ maps any configuration $\\pmb{x}$ to a real number estimating the true probability $p_{data}(\\mathbf{x})$.\n",
        "\n",
        "The maximum likelihood estimator for $\\pmb{\\theta}$ is then defined as\n",
        "\n",
        "\\begin{split}\n",
        "\\pmb{\\theta}_{\\mathrm{ML}} & = \\underset{\\pmb{\\theta}}{\\arg\\max}p_{model}(\\mathbb{X}| \\pmb{\\theta}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}\\prod_{i=1}^{m}p_{model}(\\pmb{x}^{(i)}|\\pmb{\\theta}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}\\sum_{i=1}^{m}\\log p_{model}(\\pmb{x}^{(i)}|\\pmb{\\theta}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}\\mathbb{E}_{\\mathbf{x} \\sim \\hat{p}_{data}}\\log p_{model}(\\pmb{x}|\\pmb{\\theta})\n",
        "\\end{split}\n",
        "\n",
        "\n",
        "TODO 1: add a very simple case where the model is a Bernoulli distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YA5_XBwcD2C",
        "colab_type": "text"
      },
      "source": [
        "## Maximum A Posteriori (MAP) Estimation\n",
        "\n",
        "Since $p(\\pmb{\\theta}|\\mathbb{X}) = \\frac {p({\\mathbb{X}|\\pmb{\\theta}})p(\\pmb{\\theta})}{p(\\mathbb{X})}$, according to Bayes' theorem,\n",
        "\n",
        "\\begin{split}\n",
        "\\pmb{\\theta}_{\\mathrm{MAP}} & = \\underset{\\pmb{\\theta}}{\\arg\\max}p(\\pmb{\\theta} | \\mathbb{X}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max} [\\log p(\\mathbb{X}|\\pmb{\\theta}) + \\log p(\\pmb{\\theta})] \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}[\\sum_{i=1}^{m}\\log p_{model}(\\pmb{x}^{(i)}|\\pmb{\\theta}) + \\log p(\\pmb{\\theta})]\n",
        "\\end{split}\n",
        "\n",
        "MAP adds a prior to the likelihood. If the prior distribution is constant over the $\\pmb{\\theta}$ (i.e. a uniform distribution), then MAP is the same as MLE.\n",
        "\n",
        "TODO: add a simple example. Gaussian is probably the easiest. https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFmNVbYFl9Yx",
        "colab_type": "text"
      },
      "source": [
        "# MLE and MAP in supervised machine learning\n",
        "\n",
        "\n",
        "To jump to the conclusion, we can prove that the optimzation we are doing in supervised machine learning is actually also MLE or MAP. To be more precise, it's MLE if there are no weight decay regularizers added to the loss function, and if there are then it's MAP. \n",
        "\n",
        "Check out the paper [Deep Learning: A Bayesian Perspective](https://arxiv.org/abs/1706.00473) for more in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V9QmhZ5n89x",
        "colab_type": "text"
      },
      "source": [
        "## MLE with supervised learning\n",
        "\n",
        "Since this is a supervised learning, besides the data $\\mathbb{X}$, we also have labels $\\mathbb{Y}$. So the MLE becomes as below\n",
        "\n",
        "\\begin{split}\n",
        "\\pmb{\\theta}_{\\mathrm{ML}} & = \\underset{\\pmb{\\theta}}{\\arg\\max}p_{model}(\\mathbb{Y} | \\mathbb{X}, \\pmb{\\theta}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}\\prod_{i=1}^{m}p_{model}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)},\\pmb{\\theta}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}\\sum_{i=1}^{m}\\log p_{model}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)},\\pmb{\\theta})\n",
        "\\end{split}\n",
        "\n",
        "This is where it gets confusing. For MLE, we are normally used to having $\\mathbb{X}$ before the vertical bar, not after. But now $\\mathbb{X}$ is after the bar! This bothered me a lot in the beginning but I noticed that it doesn't really matter. \n",
        "\n",
        "I tend to think $p_{model}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)},\\pmb{\\theta})$ as $p_{model}((\\pmb{y},\\pmb{x})^{(i)}|\\pmb{\\theta})$. I'm not sure if this makes sense mathematically, but it makes more sense for me. Every data sample $\\pmb{x}^{(i)}$in the training data set has its label $\\pmb{y}^{(i)}$, and they come together. So I think it makes more sense it to write $\\pmb{y}^{(i)} | \\pmb{x}^{(i)}$ as $(\\pmb{y},\\pmb{x})^{(i)}$.  \n",
        "\n",
        "\n",
        "TODO 1: add an example of Linear Regression as Maximum Likelihood (this is in the book).\n",
        "\n",
        "TODO 2: add an example of Logistic Regression as Maximum Likelihood (https://stats.stackexchange.com/questions/402511/is-it-correct-to-say-the-neural-networks-are-an-alternative-way-of-performing-ma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLFmP6h06oE",
        "colab_type": "text"
      },
      "source": [
        "## MAP with supervised learning\n",
        "\n",
        "Since $p(\\pmb{\\theta}|\\mathbb{Y}, \\mathbb{X}) = \\frac {p({\\mathbb{Y}|\\mathbb{X}, \\pmb{\\theta}})p(\\pmb{\\theta})}{p(\\mathbb{Y} | \\mathbb{X})}$, according to Bayes' theorem,\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "\\pmb{\\theta}_{\\mathrm{MAP}} & = \\underset{\\pmb{\\theta}}{\\arg\\max}p(\\pmb{\\theta} | \\mathbb{Y}, \\mathbb{X}) \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max} [\\log p(\\mathbb{Y}|\\mathbb{X},\\pmb{\\theta}) + \\log p(\\pmb{\\theta})] \\\\\n",
        "& = \\underset{\\pmb{\\theta}}{\\arg\\max}[\\sum_{i=1}^{m}\\log p_{model}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)}, \\pmb{\\theta}) + \\log p(\\pmb{\\theta})]\n",
        "\\end{split}\n",
        "\n",
        "As said in the book, if the prior $\\pmb{\\theta}$ is a Gaussian whose mean is a zero vector and covariance matrix is a unit covariance matrix multiplied by a scalar, then $\\log p(\\pmb{\\theta})$ becomes a $L^2$ weight decay term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfJwXqScm7oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y9cdLSkmFDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}